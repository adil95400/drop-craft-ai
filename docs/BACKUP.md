# üíæ Guide de Sauvegarde & Restauration - Drop Craft AI

## Vue d'ensemble

Ce document d√©crit la strat√©gie compl√®te de sauvegarde (backup) et de restauration (recovery) pour garantir la r√©silience des donn√©es et la continuit√© de service.

---

## üéØ Objectifs

### RTO & RPO

| Composant | RTO (Recovery Time) | RPO (Recovery Point) |
|-----------|-------------------|---------------------|
| **Base de donn√©es** | < 1 heure | < 15 minutes |
| **Fichiers utilisateurs** | < 2 heures | < 1 heure |
| **Configuration** | < 30 minutes | 0 (versionn√©e Git) |
| **Code application** | < 15 minutes | 0 (Git) |

**D√©finitions:**
- **RTO**: Temps maximum acceptable pour restaurer le service
- **RPO**: Quantit√© maximale de donn√©es acceptables √† perdre

---

## üóÑÔ∏è Strat√©gie de Sauvegarde

### 1. Base de Donn√©es Supabase

#### Backups Automatiques

**Supabase g√®re automatiquement:**
- **Daily backups**: Conserv√©s 7 jours (tous plans)
- **PITR** (Point-in-Time Recovery): Conserv√© 7-30 jours (plans Pro+)

**Configuration:**
```toml
# supabase/config.toml
[db.backup]
enabled = true
retention_days = 7

[db.pitr]
enabled = true  # Pro+ uniquement
retention_days = 30
```

#### Backups Manuels

##### Via Supabase CLI

```bash
# Backup complet
npx supabase db dump \
  --project-ref YOUR_PROJECT_REF \
  --file backup-$(date +%Y%m%d-%H%M%S).sql

# Backup structure seule (sans donn√©es)
npx supabase db dump \
  --project-ref YOUR_PROJECT_REF \
  --schema-only \
  --file schema-$(date +%Y%m%d).sql

# Backup donn√©es seules
npx supabase db dump \
  --project-ref YOUR_PROJECT_REF \
  --data-only \
  --file data-$(date +%Y%m%d).sql
```

##### Via pg_dump (Avanc√©)

```bash
# Connexion directe avec pg_dump
pg_dump \
  --host db.YOUR_PROJECT_REF.supabase.co \
  --port 5432 \
  --username postgres \
  --dbname postgres \
  --format custom \
  --file backup.dump

# Avec compression
pg_dump \
  --host db.YOUR_PROJECT_REF.supabase.co \
  --port 5432 \
  --username postgres \
  --dbname postgres \
  --format custom \
  --compress 9 \
  --file backup-$(date +%Y%m%d).dump.gz
```

#### Script de Backup Automatis√©

```bash
#!/bin/bash
# scripts/backup-database.sh

set -e

PROJECT_REF="YOUR_PROJECT_REF"
BACKUP_DIR="./backups/database"
DATE=$(date +%Y%m%d-%H%M%S)
RETENTION_DAYS=30

# Cr√©er r√©pertoire
mkdir -p "$BACKUP_DIR"

echo "üîÑ Starting database backup..."

# Backup complet
npx supabase db dump \
  --project-ref "$PROJECT_REF" \
  --file "$BACKUP_DIR/backup-$DATE.sql"

# Compression
gzip "$BACKUP_DIR/backup-$DATE.sql"

echo "‚úÖ Backup completed: backup-$DATE.sql.gz"

# Upload vers S3/Backblaze/etc.
if [ -n "$AWS_S3_BUCKET" ]; then
  aws s3 cp \
    "$BACKUP_DIR/backup-$DATE.sql.gz" \
    "s3://$AWS_S3_BUCKET/backups/database/"
  echo "‚òÅÔ∏è Uploaded to S3"
fi

# Nettoyage anciens backups locaux
find "$BACKUP_DIR" -name "backup-*.sql.gz" -mtime +$RETENTION_DAYS -delete
echo "üßπ Cleaned old backups (>$RETENTION_DAYS days)"

echo "‚ú® Backup process completed successfully"
```

**Cron Job:**
```bash
# Ex√©cuter tous les jours √† 2h du matin
0 2 * * * /path/to/scripts/backup-database.sh >> /var/log/backup.log 2>&1
```

#### Backup de Tables Sp√©cifiques

```bash
# Backup d'une table critique
npx supabase db dump \
  --project-ref YOUR_PROJECT_REF \
  --table products \
  --file products-backup-$(date +%Y%m%d).sql

# Backup de plusieurs tables
npx supabase db dump \
  --project-ref YOUR_PROJECT_REF \
  --table products \
  --table orders \
  --table users \
  --file critical-tables-$(date +%Y%m%d).sql
```

### 2. Fichiers & Storage

#### Supabase Storage

##### Backup via API

```typescript
// scripts/backup-storage.ts
import { createClient } from '@supabase/supabase-js';
import * as fs from 'fs';
import * as path from 'path';

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_KEY!
);

async function backupBucket(bucketName: string) {
  console.log(`üì¶ Backing up bucket: ${bucketName}`);
  
  // Lister tous les fichiers
  const { data: files, error } = await supabase
    .storage
    .from(bucketName)
    .list();
  
  if (error) throw error;
  
  const backupDir = `./backups/storage/${bucketName}`;
  fs.mkdirSync(backupDir, { recursive: true });
  
  // T√©l√©charger chaque fichier
  for (const file of files) {
    const { data, error } = await supabase
      .storage
      .from(bucketName)
      .download(file.name);
    
    if (error) {
      console.error(`‚ùå Error downloading ${file.name}:`, error);
      continue;
    }
    
    const buffer = Buffer.from(await data.arrayBuffer());
    fs.writeFileSync(
      path.join(backupDir, file.name),
      buffer
    );
    
    console.log(`‚úÖ Downloaded: ${file.name}`);
  }
  
  console.log(`‚ú® Backup completed for bucket: ${bucketName}`);
}

// Backup tous les buckets
async function backupAllStorage() {
  const buckets = ['avatars', 'product-images', 'documents'];
  
  for (const bucket of buckets) {
    await backupBucket(bucket);
  }
}

backupAllStorage().catch(console.error);
```

##### Script de Synchronisation

```bash
#!/bin/bash
# scripts/sync-storage.sh

# Utiliser rclone pour sync incr√©mental
rclone sync \
  supabase-storage:product-images \
  ./backups/storage/product-images \
  --progress \
  --transfers 8

rclone sync \
  supabase-storage:avatars \
  ./backups/storage/avatars \
  --progress \
  --transfers 8
```

### 3. Configuration & Secrets

#### Variables d'Environnement

```bash
# scripts/backup-env.sh

# Backup .env (ATTENTION: fichier sensible!)
cp .env "./backups/env/.env.$(date +%Y%m%d)"

# Chiffrer le backup
gpg --symmetric --cipher-algo AES256 \
  "./backups/env/.env.$(date +%Y%m%d)"

# Supprimer version non chiffr√©e
rm "./backups/env/.env.$(date +%Y%m%d)"
```

#### Secrets Supabase

```bash
# Exporter les secrets Edge Functions
npx supabase secrets list --project-ref YOUR_PROJECT_REF > secrets-backup.txt

# Chiffrer
gpg --symmetric --cipher-algo AES256 secrets-backup.txt
rm secrets-backup.txt
```

### 4. Code & Configuration Git

Le code est automatiquement versionn√© via Git, mais pensez √†:

```bash
# Tag des releases
git tag -a v1.0.0 -m "Production release v1.0.0"
git push origin v1.0.0

# Backup du repository
git bundle create drop-craft-ai-$(date +%Y%m%d).bundle --all

# Upload du bundle
aws s3 cp \
  drop-craft-ai-$(date +%Y%m%d).bundle \
  s3://backups/git/
```

---

## üîÑ Strat√©gie de Restauration

### 1. Restauration Base de Donn√©es

#### Restauration Compl√®te

```bash
# Via Supabase CLI
npx supabase db push \
  --project-ref YOUR_PROJECT_REF \
  --file backup-20240115.sql

# Via psql
psql \
  --host db.YOUR_PROJECT_REF.supabase.co \
  --port 5432 \
  --username postgres \
  --dbname postgres \
  < backup-20240115.sql
```

#### Restauration Point-in-Time (PITR)

**Via Supabase Dashboard:**
1. Database ‚Üí Backups ‚Üí Point in Time Recovery
2. S√©lectionner la date/heure
3. Cliquer "Restore"
4. ‚ö†Ô∏è Cr√©era un nouveau projet

**Via CLI (bient√¥t disponible):**
```bash
npx supabase db restore \
  --project-ref YOUR_PROJECT_REF \
  --timestamp "2024-01-15 14:30:00"
```

#### Restauration Table Sp√©cifique

```sql
-- 1. Cr√©er une table temporaire depuis le backup
CREATE TABLE products_backup AS 
SELECT * FROM products;

-- 2. Restaurer depuis le backup
DROP TABLE products;
CREATE TABLE products (...);  -- structure depuis backup
\COPY products FROM 'products-backup.csv' WITH CSV HEADER;

-- 3. V√©rifier
SELECT COUNT(*) FROM products;
SELECT COUNT(*) FROM products_backup;
```

### 2. Restauration Storage

```typescript
// scripts/restore-storage.ts
import { createClient } from '@supabase/supabase-js';
import * as fs from 'fs';
import * as path from 'path';

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_KEY!
);

async function restoreBucket(bucketName: string) {
  console.log(`üì¶ Restoring bucket: ${bucketName}`);
  
  const backupDir = `./backups/storage/${bucketName}`;
  const files = fs.readdirSync(backupDir);
  
  for (const filename of files) {
    const filePath = path.join(backupDir, filename);
    const fileBuffer = fs.readFileSync(filePath);
    
    const { error } = await supabase
      .storage
      .from(bucketName)
      .upload(filename, fileBuffer, {
        upsert: true
      });
    
    if (error) {
      console.error(`‚ùå Error uploading ${filename}:`, error);
      continue;
    }
    
    console.log(`‚úÖ Uploaded: ${filename}`);
  }
  
  console.log(`‚ú® Restore completed for bucket: ${bucketName}`);
}

restoreBucket('product-images').catch(console.error);
```

### 3. Restauration apr√®s Incident

#### Proc√©dure Compl√®te

```bash
#!/bin/bash
# scripts/disaster-recovery.sh

set -e

echo "üö® Starting Disaster Recovery Process..."

# 1. V√©rifier les backups disponibles
echo "üìã Available backups:"
ls -lh ./backups/database/ | tail -5

# 2. Demander confirmation
read -p "Enter backup filename to restore: " BACKUP_FILE
read -p "‚ö†Ô∏è  This will overwrite current data. Continue? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
  echo "‚ùå Recovery cancelled"
  exit 1
fi

# 3. Cr√©er un backup de s√©curit√© avant restauration
echo "üíæ Creating safety backup..."
npx supabase db dump \
  --project-ref $PROJECT_REF \
  --file "./backups/safety-backup-$(date +%Y%m%d-%H%M%S).sql"

# 4. Restaurer la base de donn√©es
echo "üîÑ Restoring database..."
npx supabase db push \
  --project-ref $PROJECT_REF \
  --file "./backups/database/$BACKUP_FILE"

# 5. Restaurer le storage
echo "üì¶ Restoring storage..."
./scripts/restore-storage.sh

# 6. V√©rifications
echo "‚úÖ Running post-restore checks..."
npx supabase db test

# 7. Notification
echo "üìß Sending notification..."
curl -X POST $SLACK_WEBHOOK \
  -H 'Content-Type: application/json' \
  -d "{\"text\":\"‚úÖ Disaster recovery completed for $BACKUP_FILE\"}"

echo "‚ú® Disaster Recovery Process Completed!"
```

---

## üß™ Tests de Restauration

### Plan de Test Mensuel

```bash
#!/bin/bash
# scripts/test-restore.sh

# 1. Cr√©er un environnement de test
echo "üß™ Creating test environment..."
npx supabase db reset --test-environment

# 2. Restaurer dernier backup
LATEST_BACKUP=$(ls -t ./backups/database/*.sql.gz | head -1)
echo "üì¶ Testing restore of: $LATEST_BACKUP"

gunzip -c "$LATEST_BACKUP" | \
  npx supabase db push --test-environment

# 3. V√©rifications
echo "‚úÖ Running validation tests..."

# V√©rifier nombre de tables
TABLE_COUNT=$(psql --test-environment -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'")
echo "Tables restored: $TABLE_COUNT"

# V√©rifier donn√©es critiques
PRODUCT_COUNT=$(psql --test-environment -t -c "SELECT COUNT(*) FROM products")
echo "Products restored: $PRODUCT_COUNT"

USER_COUNT=$(psql --test-environment -t -c "SELECT COUNT(*) FROM profiles")
echo "Users restored: $USER_COUNT"

# 4. Rapport
echo "üìä Restore Test Report"
echo "===================="
echo "Backup tested: $LATEST_BACKUP"
echo "Status: ‚úÖ SUCCESS"
echo "Tables: $TABLE_COUNT"
echo "Products: $PRODUCT_COUNT"
echo "Users: $USER_COUNT"
echo "===================="

# 5. Nettoyage
npx supabase db destroy --test-environment
```

**Cron Job (1er de chaque mois):**
```bash
0 3 1 * * /path/to/scripts/test-restore.sh >> /var/log/restore-test.log 2>&1
```

---

## üì¶ Stockage des Backups

### Strat√©gie 3-2-1

**Recommandation:**
- **3** copies des donn√©es
- Sur **2** supports diff√©rents
- **1** copie hors site

#### Configuration

```bash
# 1. Local (NAS, disque externe)
BACKUP_LOCAL="/mnt/nas/backups/drop-craft-ai"

# 2. Cloud (S3, Backblaze B2)
BACKUP_CLOUD="s3://company-backups/drop-craft-ai"

# 3. Offsite (autre r√©gion cloud)
BACKUP_OFFSITE="s3://eu-west-1/company-backups-dr/drop-craft-ai"
```

#### Script de Distribution

```bash
#!/bin/bash
# scripts/distribute-backups.sh

BACKUP_FILE="backup-$(date +%Y%m%d).sql.gz"

# Upload vers Cloud primary
aws s3 cp "$BACKUP_FILE" "$BACKUP_CLOUD/"

# Upload vers Cloud offsite
aws s3 cp "$BACKUP_FILE" "$BACKUP_OFFSITE/" --region eu-west-1

# Copie locale
cp "$BACKUP_FILE" "$BACKUP_LOCAL/"

echo "‚úÖ Backup distributed to 3 locations"
```

### Providers Recommand√©s

| Provider | Prix | Avantages |
|----------|------|-----------|
| **AWS S3** | ~$0.023/GB/mois | Int√©gration native, versioning |
| **Backblaze B2** | $0.005/GB/mois | √âconomique, S3-compatible |
| **Google Cloud Storage** | ~$0.020/GB/mois | Performance, multi-r√©gion |
| **Azure Blob Storage** | ~$0.018/GB/mois | Int√©gration Microsoft |

---

## üîê S√©curit√© des Backups

### Chiffrement

```bash
# Chiffrer avec GPG
gpg --symmetric --cipher-algo AES256 backup.sql
# Cr√©√©: backup.sql.gpg

# D√©chiffrer
gpg --decrypt backup.sql.gpg > backup.sql
```

### Gestion des Cl√©s

```bash
# G√©n√©rer une cl√© de chiffrement
openssl rand -base64 32 > backup-encryption.key

# Stocker la cl√© de mani√®re s√©curis√©e
# - Gestionnaire de mots de passe (1Password, LastPass)
# - Vault (HashiCorp)
# - AWS Secrets Manager
# - JAMAIS dans Git!
```

### Permissions d'Acc√®s

```bash
# Backups accessibles uniquement par le script
chmod 700 ./backups/
chmod 600 ./backups/database/*
chmod 600 ./backups/env/*

# Logs accessibles en lecture
chmod 644 /var/log/backup.log
```

---

## üìä Monitoring des Backups

### V√©rifications Automatiques

```typescript
// Edge Function: verify-backups
import { serve } from 'https://deno.land/std/http/server.ts';

serve(async () => {
  const checks = {
    databaseBackup: await checkDatabaseBackup(),
    storageBackup: await checkStorageBackup(),
    offlineBackup: await checkOfflineBackup()
  };
  
  const allOk = Object.values(checks).every(check => check.status === 'ok');
  
  if (!allOk) {
    await sendAlert({
      severity: 'high',
      title: 'Backup Verification Failed',
      details: checks
    });
  }
  
  return new Response(JSON.stringify(checks), {
    status: allOk ? 200 : 500
  });
});

async function checkDatabaseBackup() {
  // V√©rifier que le dernier backup a < 24h
  const lastBackup = await getLastBackupTimestamp();
  const ageHours = (Date.now() - lastBackup) / (1000 * 60 * 60);
  
  return {
    status: ageHours < 24 ? 'ok' : 'failed',
    lastBackup: new Date(lastBackup).toISOString(),
    ageHours: ageHours.toFixed(2)
  };
}
```

### Dashboard de Backups

```typescript
// src/pages/admin/backup-dashboard.tsx
export function BackupDashboard() {
  return (
    <div className="space-y-4">
      <Card>
        <CardHeader>Backup Status</CardHeader>
        <CardContent>
          <BackupStatusGrid>
            <BackupStatus
              type="Database"
              lastRun="2024-01-15 02:00"
              status="success"
              size="2.4 GB"
            />
            <BackupStatus
              type="Storage"
              lastRun="2024-01-15 03:00"
              status="success"
              size="15.8 GB"
            />
            <BackupStatus
              type="Offsite"
              lastRun="2024-01-15 04:00"
              status="success"
              size="18.2 GB"
            />
          </BackupStatusGrid>
        </CardContent>
      </Card>
      
      <Card>
        <CardHeader>Recent Backups</CardHeader>
        <CardContent>
          <BackupHistoryTable backups={recentBackups} />
        </CardContent>
      </Card>
      
      <Card>
        <CardHeader>Quick Actions</CardHeader>
        <CardContent>
          <Button onClick={triggerBackup}>
            Trigger Manual Backup
          </Button>
          <Button onClick={testRestore}>
            Test Restore
          </Button>
        </CardContent>
      </Card>
    </div>
  );
}
```

---

## üìã Checklist de Backup

### Quotidienne
- [ ] Backup automatique DB ex√©cut√©
- [ ] Backup storage synchronis√©
- [ ] Logs de backup v√©rifi√©s
- [ ] Espace disque suffisant

### Hebdomadaire
- [ ] Backup offsite v√©rifi√©
- [ ] Nettoyage anciens backups
- [ ] Test de restauration table
- [ ] Rapport envoy√©

### Mensuelle
- [ ] Test de restauration compl√®te
- [ ] V√©rification chiffrement
- [ ] Audit acc√®s backups
- [ ] Mise √† jour documentation

---

## üÜò Contacts d'Urgence

| Situation | Contact | T√©l√©phone |
|-----------|---------|-----------|
| **Perte de donn√©es** | ops@drop-craft-ai.com | +33 1 XX XX XX XX |
| **Support Supabase** | support.supabase.com | - |
| **Oncall Engineer** | oncall@drop-craft-ai.com | +33 6 XX XX XX XX |

---

## üìö Ressources

- [Supabase Backup Guide](https://supabase.com/docs/guides/platform/backups)
- [PostgreSQL Backup Best Practices](https://www.postgresql.org/docs/current/backup.html)
- [AWS S3 Backup Strategies](https://aws.amazon.com/backup/)
- [3-2-1 Backup Rule](https://www.backblaze.com/blog/the-3-2-1-backup-strategy/)

---

**Derni√®re mise √† jour**: 2024-01-XX  
**Responsable Backups**: ops@drop-craft-ai.com  
**Prochaine r√©vision**: 2024-02-01
